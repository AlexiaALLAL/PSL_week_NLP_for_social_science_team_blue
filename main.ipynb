{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>album</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>n_words</th>\n",
       "      <th>pageviews</th>\n",
       "      <th>contributors</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Dry</td>\n",
       "      <td>94310</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Tôt ou tard</td>\n",
       "      <td>[Paroles de \"94310\" ft. Madj, Larso &amp; Stokas]\\...</td>\n",
       "      <td>748</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>https://genius.com/Dry-94310-lyrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Mafia K’1 Fry</td>\n",
       "      <td>Au bon vieux temps</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>Jusqu’à la mort</td>\n",
       "      <td>\\nEt je les entends tous les Zoulous, ils parl...</td>\n",
       "      <td>958</td>\n",
       "      <td>7514</td>\n",
       "      <td>8</td>\n",
       "      <td>https://genius.com/Mafia-k1-fry-au-bon-vieux-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>DJ Hamida</td>\n",
       "      <td>Attrape-Moi Si Tu Peux</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mix Party 2015</td>\n",
       "      <td>[Refrain : Charly Bell]\\nOui je sais qu’t’as e...</td>\n",
       "      <td>367</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>https://genius.com/Dj-hamida-attrape-moi-si-tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Kery James</td>\n",
       "      <td>94 c’est le Barça Remix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nC'est pour les rudeboys, c'est pour les cail...</td>\n",
       "      <td>535</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>https://genius.com/Kery-james-94-cest-le-barca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Dry</td>\n",
       "      <td>14 ans déjà</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>Maintenant ou jamais</td>\n",
       "      <td>[Paroles de \"14 ans déjà\"]\\n\\n\\nTu sais, j'cro...</td>\n",
       "      <td>653</td>\n",
       "      <td>5037</td>\n",
       "      <td>8</td>\n",
       "      <td>https://genius.com/Dry-14-ans-deja-lyrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85018</th>\n",
       "      <td>85019</td>\n",
       "      <td>Bricksy &amp; 3g</td>\n",
       "      <td>Akai</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>But It Ain’t</td>\n",
       "      <td>[Paroles de \"Akai\" ft. La Fève]\\n\\n\\nFF7\\nWalo...</td>\n",
       "      <td>471</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>https://genius.com/Bricksy-and-3g-akai-lyrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85019</th>\n",
       "      <td>85020</td>\n",
       "      <td>Bricksy &amp; 3g</td>\n",
       "      <td>I am GIA</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>But It Ain’t</td>\n",
       "      <td>[Paroles de \"I am GIA\"]\\n\\n\\nSa star\\nIl me di...</td>\n",
       "      <td>522</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>https://genius.com/Bricksy-and-3g-i-am-gia-lyrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85020</th>\n",
       "      <td>85021</td>\n",
       "      <td>Rafal</td>\n",
       "      <td>ROMEO</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n\\nPrépare le sparadrap\\nJ'péterais ma tête...</td>\n",
       "      <td>393</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>https://genius.com/Rafal-romeo-lyrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85021</th>\n",
       "      <td>85022</td>\n",
       "      <td>Aktéfräzé</td>\n",
       "      <td>Alpha, kilo, tango, [é]cho... Triple comme au ...</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>B.O.S.S. Vol. 1</td>\n",
       "      <td>\\nComme au scrabble et...\\n\\n\\nLa fine fleur d...</td>\n",
       "      <td>464</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://genius.com/Aktefraze-alpha-kilo-tango-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85022</th>\n",
       "      <td>85023</td>\n",
       "      <td>Génération Goldman</td>\n",
       "      <td>Bonne idée</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Génération Goldman Vol.2</td>\n",
       "      <td>Un début de janvier, si j'ai bien su compter\\n...</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://genius.com/Generation-goldman-bonne-id...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85023 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0              artist  \\\n",
       "0               1                 Dry   \n",
       "1               2       Mafia K’1 Fry   \n",
       "2               3           DJ Hamida   \n",
       "3               4          Kery James   \n",
       "4               5                 Dry   \n",
       "...           ...                 ...   \n",
       "85018       85019        Bricksy & 3g   \n",
       "85019       85020        Bricksy & 3g   \n",
       "85020       85021               Rafal   \n",
       "85021       85022           Aktéfräzé   \n",
       "85022       85023  Génération Goldman   \n",
       "\n",
       "                                                   title    year  \\\n",
       "0                                                  94310  2012.0   \n",
       "1                                     Au bon vieux temps  2007.0   \n",
       "2                                 Attrape-Moi Si Tu Peux     NaN   \n",
       "3                                94 c’est le Barça Remix     NaN   \n",
       "4                                            14 ans déjà  2013.0   \n",
       "...                                                  ...     ...   \n",
       "85018                                               Akai  2024.0   \n",
       "85019                                           I am GIA  2024.0   \n",
       "85020                                              ROMEO  2024.0   \n",
       "85021  Alpha, kilo, tango, [é]cho... Triple comme au ...  1999.0   \n",
       "85022                                         Bonne idée     NaN   \n",
       "\n",
       "                          album  \\\n",
       "0                   Tôt ou tard   \n",
       "1               Jusqu’à la mort   \n",
       "2                Mix Party 2015   \n",
       "3                           NaN   \n",
       "4          Maintenant ou jamais   \n",
       "...                         ...   \n",
       "85018              But It Ain’t   \n",
       "85019              But It Ain’t   \n",
       "85020                       NaN   \n",
       "85021           B.O.S.S. Vol. 1   \n",
       "85022  Génération Goldman Vol.2   \n",
       "\n",
       "                                                  lyrics  n_words  pageviews  \\\n",
       "0      [Paroles de \"94310\" ft. Madj, Larso & Stokas]\\...      748          0   \n",
       "1      \\nEt je les entends tous les Zoulous, ils parl...      958       7514   \n",
       "2      [Refrain : Charly Bell]\\nOui je sais qu’t’as e...      367          0   \n",
       "3      \\nC'est pour les rudeboys, c'est pour les cail...      535          0   \n",
       "4      [Paroles de \"14 ans déjà\"]\\n\\n\\nTu sais, j'cro...      653       5037   \n",
       "...                                                  ...      ...        ...   \n",
       "85018  [Paroles de \"Akai\" ft. La Fève]\\n\\n\\nFF7\\nWalo...      471          0   \n",
       "85019  [Paroles de \"I am GIA\"]\\n\\n\\nSa star\\nIl me di...      522          0   \n",
       "85020  \\n\\n\\nPrépare le sparadrap\\nJ'péterais ma tête...      393          0   \n",
       "85021  \\nComme au scrabble et...\\n\\n\\nLa fine fleur d...      464          0   \n",
       "85022  Un début de janvier, si j'ai bien su compter\\n...      271          0   \n",
       "\n",
       "       contributors                                                url  \n",
       "0                 8                https://genius.com/Dry-94310-lyrics  \n",
       "1                 8  https://genius.com/Mafia-k1-fry-au-bon-vieux-t...  \n",
       "2                 4  https://genius.com/Dj-hamida-attrape-moi-si-tu...  \n",
       "3                 7  https://genius.com/Kery-james-94-cest-le-barca...  \n",
       "4                 8          https://genius.com/Dry-14-ans-deja-lyrics  \n",
       "...             ...                                                ...  \n",
       "85018             7      https://genius.com/Bricksy-and-3g-akai-lyrics  \n",
       "85019             4  https://genius.com/Bricksy-and-3g-i-am-gia-lyrics  \n",
       "85020             4              https://genius.com/Rafal-romeo-lyrics  \n",
       "85021             1  https://genius.com/Aktefraze-alpha-kilo-tango-...  \n",
       "85022             2  https://genius.com/Generation-goldman-bonne-id...  \n",
       "\n",
       "[85023 rows x 10 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('corpus.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sexism analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface model :  https://huggingface.co/annahaz/xlm-roberta-base-misogyny-sexism-indomain-mix-bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda\\envs\\env_1\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"annahaz/xlm-roberta-base-misogyny-sexism-indomain-mix-bal\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"annahaz/xlm-roberta-base-misogyny-sexism-indomain-mix-bal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.7519, -3.4935]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = 'Ceci est une phrase neutre'\n",
    "# text = 'Bonjour, je vous trouve très jolie'\n",
    "text = 'T\\'es bonne ouech'\n",
    "# text = 'Chez moi, que de la bonne substance, normal elle sort d\\'Amsterdam'\n",
    "# text = 'C\\'est une salope'\n",
    "# text = 'Je vais te la mettre si t\\'ouvres tes fesses'\n",
    "\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "print(logits)\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "# model.config.id2label[predicted_class_id]\n",
    "predicted_class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.6209, -2.1076]]), 0.21875)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # text = text.replace('\\n', ' ')\n",
    "    # text = text.replace('\\r', ' ')\n",
    "    text = re.sub(r'\\[.*?\\]', '', text) # remove text between brackets (title)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text) # remove punctuation\n",
    "    return text\n",
    "\n",
    "def violence_probability(line):\n",
    "    inputs = tokenizer(line, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    # predicted_class_id = logits.argmax().item()\n",
    "    # return model.config.id2label[predicted_class_id]\n",
    "    return logits\n",
    "\n",
    "def categorize(lyrics):\n",
    "    # for each line of the lyrics, get the probability of violence and return the mean\n",
    "    lyrics = clean_text(lyrics)\n",
    "    lines = lyrics.split('\\n')\n",
    "    lines = [line for line in lines if line] # remove empty lines\n",
    "    # print(lines)\n",
    "    probabilities = [violence_probability(line) for line in lines]\n",
    "    # print(probabilities)\n",
    "\n",
    "    mean_probability = sum(probabilities) / len(probabilities)\n",
    "    # print('mean probability : ', mean_probability)    \n",
    "    predicted_class_id = mean_probability.argmax().item()\n",
    "    mean_proba_classe = model.config.id2label[predicted_class_id]\n",
    "\n",
    "    # get the class of each line\n",
    "    classes = [model.config.id2label[proba.argmax().item()] for proba in probabilities]\n",
    "    # print(classes)\n",
    "    nb_violent_lines = len([cl for cl in classes if cl == '1'])\n",
    "    nb_non_violent_lines = len([cl for cl in classes if cl == '0'])\n",
    "    ratio_violent_lines = nb_violent_lines / (nb_violent_lines + nb_non_violent_lines)\n",
    "    # print('ratio violent lines : ', ratio_violent_lines)\n",
    "\n",
    "    return mean_probability, ratio_violent_lines\n",
    "\n",
    "lyrics = df['lyrics'][0]\n",
    "categorize(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.1422, -0.0194]]), 0.5888888888888889)\n",
      "(tensor([[ 0.8407, -1.2274]]), 0.43283582089552236)\n",
      "(tensor([[ 1.1938, -1.5892]]), 0.25)\n",
      "(tensor([[ 1.5284, -1.9794]]), 0.24528301886792453)\n",
      "(tensor([[ 1.2478, -1.6807]]), 0.28735632183908044)\n",
      "(tensor([[ 1.5624, -2.0617]]), 0.18556701030927836)\n",
      "(tensor([[ 0.5954, -0.9085]]), 0.4074074074074074)\n",
      "(tensor([[ 1.1115, -1.5045]]), 0.2807017543859649)\n",
      "(tensor([[-0.6203,  0.5877]]), 0.6111111111111112)\n",
      "(tensor([[ 1.7283, -2.2697]]), 0.16666666666666666)\n"
     ]
    }
   ],
   "source": [
    "df_alkpote = df[df['artist'] == 'Alkpote']\n",
    "for i in range(10):\n",
    "    lyrics = df_alkpote.iloc[i]['lyrics']\n",
    "    # print('lyrics : ', lyrics)\n",
    "    print(categorize(lyrics))\n",
    "    # print('\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'artist', 'title', 'year', 'album', 'lyrics', 'n_words',\n",
       "       'pageviews', 'contributors', 'url', 'clean_lyrics', 'tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_popular_artists = df[['artist', 'pageviews']].groupby('artist').sum().sort_values(by='pageviews', ascending=False).head(10)\n",
    "df[df['artist'].isin(most_popular_artists.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist :  Booba\n",
      "song :  0\n",
      "(tensor([[ 1.7870, -2.3188]]), 0.21212121212121213)\n",
      "song :  1\n",
      "(tensor([[ 1.4541, -1.9047]]), 0.23076923076923078)\n",
      "song :  2\n",
      "(tensor([[ 1.1447, -1.5200]]), 0.2926829268292683)\n",
      "song :  3\n",
      "(tensor([[ 1.2992, -1.7314]]), 0.225)\n",
      "song :  4\n",
      "(tensor([[ 1.6250, -2.1585]]), 0.2)\n",
      "song :  5\n",
      "(tensor([[ 2.4419, -3.0426]]), 0.07894736842105263)\n",
      "song :  6\n",
      "(tensor([[ 2.0552, -2.6371]]), 0.13333333333333333)\n",
      "song :  7\n",
      "(tensor([[ 3.4965, -4.1965]]), 0.0)\n",
      "song :  8\n",
      "(tensor([[ 0.9775, -1.3180]]), 0.358974358974359)\n",
      "song :  9\n",
      "(tensor([[ 1.8567, -2.4071]]), 0.17647058823529413)\n",
      "song :  10\n",
      "(tensor([[ 0.4215, -0.6822]]), 0.41025641025641024)\n",
      "song :  11\n",
      "(tensor([[ 1.6453, -2.1467]]), 0.2222222222222222)\n",
      "song :  12\n",
      "(tensor([[ 0.4174, -0.6488]]), 0.4166666666666667)\n",
      "song :  13\n",
      "(tensor([[ 1.1362, -1.5420]]), 0.3048780487804878)\n",
      "song :  14\n",
      "(tensor([[ 0.8337, -1.1618]]), 0.3472222222222222)\n",
      "song :  15\n",
      "(tensor([[ 1.3778, -1.7671]]), 0.26785714285714285)\n",
      "song :  16\n",
      "(tensor([[ 2.0304, -2.5669]]), 0.136986301369863)\n",
      "song :  17\n",
      "(tensor([[ 1.3991, -1.8655]]), 0.25)\n",
      "song :  18\n",
      "(tensor([[ 0.6524, -0.9485]]), 0.34782608695652173)\n",
      "song :  19\n",
      "(tensor([[ 1.6535, -2.1419]]), 0.21818181818181817)\n",
      "song :  20\n",
      "(tensor([[ 2.0916, -2.6276]]), 0.14814814814814814)\n",
      "song :  21\n",
      "(tensor([[ 1.6480, -2.1367]]), 0.1875)\n",
      "song :  22\n",
      "(tensor([[ 1.7982, -2.2901]]), 0.19642857142857142)\n",
      "song :  23\n",
      "(tensor([[ 2.8429, -3.4847]]), 0.05172413793103448)\n",
      "song :  24\n",
      "(tensor([[ 1.4813, -1.9256]]), 0.23636363636363636)\n",
      "song :  25\n",
      "(tensor([[ 1.5298, -1.9636]]), 0.25)\n",
      "song :  26\n",
      "(tensor([[ 1.9276, -2.4218]]), 0.18461538461538463)\n",
      "song :  27\n",
      "(tensor([[ 0.4678, -0.7594]]), 0.46153846153846156)\n",
      "song :  28\n",
      "(tensor([[ 0.9402, -1.3146]]), 0.3333333333333333)\n",
      "song :  29\n",
      "(tensor([[ 1.0953, -1.4520]]), 0.3170731707317073)\n",
      "song :  30\n",
      "(tensor([[ 1.8027, -2.3050]]), 0.175)\n",
      "song :  31\n",
      "(tensor([[ 1.3894, -1.7724]]), 0.26)\n",
      "song :  32\n",
      "(tensor([[ 1.4252, -1.8905]]), 0.23333333333333334)\n",
      "song :  33\n",
      "(tensor([[ 0.7776, -1.1048]]), 0.3584905660377358)\n",
      "song :  34\n",
      "(tensor([[ 1.3384, -1.7324]]), 0.2549019607843137)\n",
      "song :  35\n",
      "(tensor([[ 0.5420, -0.8668]]), 0.42424242424242425)\n",
      "song :  36\n",
      "(tensor([[ 1.5341, -2.0169]]), 0.2535211267605634)\n",
      "song :  37\n",
      "(tensor([[ 1.2007, -1.5537]]), 0.35135135135135137)\n",
      "song :  38\n",
      "(tensor([[ 1.7903, -2.2822]]), 0.19696969696969696)\n",
      "song :  39\n",
      "(tensor([[ 1.0877, -1.4670]]), 0.31363636363636366)\n",
      "song :  40\n",
      "(tensor([[ 1.8913, -2.3985]]), 0.19230769230769232)\n",
      "song :  41\n",
      "(tensor([[ 1.9261, -2.4648]]), 0.16666666666666666)\n",
      "song :  42\n",
      "(tensor([[ 0.6186, -0.9186]]), 0.40476190476190477)\n",
      "song :  43\n",
      "(tensor([[ 1.1098, -1.4831]]), 0.3333333333333333)\n",
      "song :  44\n",
      "(tensor([[ 2.0627, -2.6422]]), 0.125)\n",
      "song :  45\n",
      "(tensor([[ 0.6939, -0.9730]]), 0.4230769230769231)\n",
      "song :  46\n",
      "(tensor([[ 3.3191, -4.0470]]), 0.0)\n",
      "song :  47\n",
      "(tensor([[ 1.3859, -1.8261]]), 0.25675675675675674)\n",
      "song :  48\n",
      "(tensor([[ 1.2865, -1.7165]]), 0.23529411764705882)\n",
      "song :  49\n",
      "(tensor([[ 2.0277, -2.5256]]), 0.19607843137254902)\n",
      "song :  50\n",
      "(tensor([[ 1.8749, -2.3651]]), 0.1951219512195122)\n",
      "song :  51\n",
      "(tensor([[ 0.9249, -1.3182]]), 0.26666666666666666)\n",
      "song :  52\n",
      "(tensor([[ 1.3282, -1.7446]]), 0.2857142857142857)\n",
      "song :  53\n",
      "(tensor([[ 1.2331, -1.6427]]), 0.2857142857142857)\n",
      "song :  54\n",
      "(tensor([[ 2.0244, -2.5584]]), 0.125)\n",
      "song :  55\n",
      "(tensor([[ 0.7808, -1.0707]]), 0.36363636363636365)\n",
      "song :  56\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m lyrics \u001b[38;5;241m=\u001b[39m df_artist\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlyrics\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# print('lyrics : ', lyrics)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcategorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlyrics\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m, in \u001b[0;36mcategorize\u001b[1;34m(lyrics)\u001b[0m\n\u001b[0;32m     24\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m [violence_probability(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# print(probabilities)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m mean_probability \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# print('mean probability : ', mean_probability)    \u001b[39;00m\n\u001b[0;32m     29\u001b[0m predicted_class_id \u001b[38;5;241m=\u001b[39m mean_probability\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# get 10 most popular artists (by number of pageviews on total on genius)\n",
    "\n",
    "most_popular_artists = df[['artist', 'pageviews']].groupby('artist').sum().sort_values(by='pageviews', ascending=False).head(10)\n",
    "\n",
    "# apply categorize function to each artist\n",
    "for artist in most_popular_artists.index:\n",
    "    print('artist : ', artist)\n",
    "    df_artist = df[df['artist'] == artist]\n",
    "    for i in range(len(df_artist)):\n",
    "        print('song : ', i)\n",
    "        lyrics = df_artist.iloc[i]['lyrics']\n",
    "        # print('lyrics : ', lyrics)\n",
    "        print(categorize(lyrics))\n",
    "        # print('\\n\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\alexi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Une',\n",
       " 'phrase',\n",
       " 'en',\n",
       " 'français',\n",
       " 'et',\n",
       " 'avec',\n",
       " 'un',\n",
       " 'accent',\n",
       " 'étizofnoz',\n",
       " '.',\n",
       " 'Une',\n",
       " 'balle',\n",
       " 'dans',\n",
       " 'la',\n",
       " 'tête']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sample_text = \"Une phrase en français et avec un accent étizofnoz. Une balle dans la tête\"\n",
    "tokens = word_tokenize(sample_text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'sample', 'text']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# text_to_tokenize = 'This is a sample text'\n",
    "# tokens = tokenizer.tokenize(text_to_tokenize)\n",
    "# tokens\n",
    "\n",
    "# df['tokens'] = df['lyrics'].apply(tokenizer.tokenize)\n",
    "# df['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    # remove punctuation and replace by space\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [Paroles de \"94310\" ft. Madj, Larso & Stokas]\\...\n",
       "1        \\nEt je les entends tous les Zoulous, ils parl...\n",
       "2        [Refrain : Charly Bell]\\nOui je sais qu’t’as e...\n",
       "3        \\nC'est pour les rudeboys, c'est pour les cail...\n",
       "4        [Paroles de \"14 ans déjà\"]\\n\\n\\nTu sais, j'cro...\n",
       "                               ...                        \n",
       "85018    [Paroles de \"Akai\" ft. La Fève]\\n\\n\\nFF7\\nWalo...\n",
       "85019    [Paroles de \"I am GIA\"]\\n\\n\\nSa star\\nIl me di...\n",
       "85020    \\n\\n\\nPrépare le sparadrap\\nJ'péterais ma tête...\n",
       "85021    \\nComme au scrabble et...\\n\\n\\nLa fine fleur d...\n",
       "85022    Un début de janvier, si j'ai bien su compter\\n...\n",
       "Name: lyrics, Length: 85023, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [Un, balle, dans, la, tête, je, te, la, place,...\n",
       "1        [Et, je, les, entends, tous, les, Zoulous, ils...\n",
       "2        [Oui, je, sais, qu, t, as, envie, On, cherche,...\n",
       "3        [C, est, pour, les, rudeboys, c, est, pour, le...\n",
       "4        [Tu, sais, j, crois, qu, j, ai, pas, réalisé, ...\n",
       "                               ...                        \n",
       "85018    [FF7, Walone, J, voulais, juste, baiser, le, g...\n",
       "85019    [Sa, star, Il, me, dit, qu, j, suis, sa, star,...\n",
       "85020    [Prépare, le, sparadrap, J, péterais, ma, tête...\n",
       "85021    [Comme, au, scrabble, et, La, fine, fleur, des...\n",
       "85022    [Un, début, de, janvier, si, j, ai, bien, su, ...\n",
       "Name: tokens, Length: 85023, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(clean_text(str(df['lyrics'][0])))\n",
    "\n",
    "df['tokens'] = df['lyrics'].apply(str).apply(clean_text).apply(word_tokenize)\n",
    "df['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Paroles de \"94310\" ft. Madj, Larso & Stokas]\n",
      "\n",
      "\n",
      "Un balle dans la tête, je te la place comme Beckham\n",
      "Je vais te la mettre si t'ouvres tes fesses, Orly-mite déboule en bécane\n",
      "Mon son dans l'illicite comme tous ces dealers de came\n",
      "Le 9.4.310 puissance de frappe, qu'il y ait pas d'amalgame\n",
      "C'est qui qui fait du bruit, dites-moi c'est quoi ce vacarme ?\n",
      "C'est S.T.O.K.A.S le mec de tess que personne ricane\n",
      "(La Rue) pardonne pas donc forcément j'ai pas d'état d'âme\n",
      "J'suis dans la street, j'suis dans les blocs et derrière moi j'laisse deux trois cadavres\n",
      "Chez moi, que de la bonne substance, normal elle sort d'Amsterdam\n",
      "Orly un phénomène, les vrais le savent on vient vous faire mal\n",
      "J'ai le flair, je le ressens les khey comme n'importe quel animal\n",
      "J'ai les crocs, j'ai envie de tout niquer les khey donc j'resterai radical\n",
      "Le Val de Marne (Le Val de Meurtre) restera dans les annales\n",
      "On défonce au MIC bien pire encore qu'un bon joint de zamal\n",
      "C'est le 9.4.310, la zone est bouillante, la zone est dangereuse\n",
      "J'dirais même, des fois même insolente, et oui khey\n",
      "\n",
      "\n",
      "J'arrive du 9.4.310\n",
      "Mon but est de peser, vendre plus de 9.4.300 disques\n",
      "La rue m'a légué bien plus de 9.4.300 vices\n",
      "Orly Sud, pour le fric c'est 9.4.300 risques\n",
      "J'arrive du 9.4.310\n",
      "Mon but est de peser, vendre plus de 9.4.300 disques\n",
      "La rue m'a légué bien plus de 9.4.300 vices\n",
      "Orly Sud, pour le fric c'est 9.4.300 risques\n",
      "\n",
      "Orly 94 ma favelas, 300 soldats déter' à mort\n",
      "Tout s'explique dans mon blaze, Lieutenant Jack Lars' des terres hardcores\n",
      "J'suis d'Orly-mite, le collectif toujours open pour faire du tort\n",
      "On est encore plus vivants qu'avant (Dry : Pendant que d'autres nous voyaient morts)\n",
      "Comme dirait D.R, mec déter', Orly ma ville gars on sait faire\n",
      "Bientôt le projet des experts, nerveux, bruyant comme un CR\n",
      "Orly ma ville c'est Thug Life, allume quand les keufs braquent\n",
      "Charcle au visage ou sous la gorge, surtout si c'est un petit qui braille\n",
      "Orly 9.4, force de frappe donc les zamels passent à la trappe\n",
      "On arbore l'instinct de pirate, Demi-Lune Zoo ma terre natale\n",
      "On accorde peur d'importance aux falches, mais donne tout pour les vrais\n",
      "Comme Fian-so,  ou Luffy ou encore comme \n",
      "En fait, je porte la rage d'ici, un flow venu d'ailleurs\n",
      "J'représente mes frères dans l'dîn jusqu'au plus petit bicraveur\n",
      "Frérot t'es le bienvenu dans mon hood, tout ça à tes risques et périls\n",
      "C'est dit, Orly 9.4.310 banlieue Sud de Berryz, Berryz\n",
      "\n",
      "\n",
      "J'arrive du 9.4.310\n",
      "Mon but est de peser, vendre plus de 9.4.300 disques\n",
      "La rue m'a légué bien plus de 9.4.300 vices\n",
      "Orly Sud, pour le fric c'est 9.4.300 risques\n",
      "\n",
      "\n",
      "Passe le mic que j'te raisonne, là c'est la zone ouais la vraie\n",
      "Très peu d'amis j'en suis navré, le tiers à Fleury ou à Fresnes\n",
      "Zone à risque 9.4.310, un coup d'Zippo et c'est l'incendie\n",
      "On arrive et tu l'as senti comme la ppe-fra dans un chantier\n",
      "Les frères écoutez les vrais, les faux finiront par se taire\n",
      "À suivre la tendance, t'as du te faire des potes mais finalement tu t'es du-per\n",
      "Trop d'amitiés rompues, on m'appelle je réponds plus\n",
      "J'ai ma belle paire de couilles en plus nos ennemis c'est des (ahh)\n",
      "À part la hass, RAS, on s'trimballe dans la rue comme des rats\n",
      "Trop souvent le moral à zéro, mais j'reste fidèle au poste comme DR\n",
      "En échec scolaire, les minots s'croient dans un polar\n",
      "Rêvent de dollar, sous Poliakov ils ont zappé l'Coca-Cola\n",
      "Les potos prennent plus des gros chiffres, mais des grosses peines et des cavales\n",
      "Drive-by sur une grosse bécane juste à cause d'une putain, cavale banane\n",
      "Aux lèvres même quand ça va mal, sourire kabyle, quartier de Paname\n",
      "Pas mal de bananes (Dry : ça jusqu'à la sanction finale)\n",
      "\n",
      "J'arrive du 9.4.310\n",
      "Mon but est de peser, vendre plus de 9.4.300 disques\n",
      "La rue m'a légué bien plus de 9.4.300 vices\n",
      "Orly Sud, pour le fric c'est 9.4.300 risques\n",
      "clean text\n",
      "\n",
      "\n",
      "\n",
      "    Un balle dans la tête  je te la place comme Beckham Je vais te la mettre si t ouvres tes fesses  Orly mite déboule en bécane Mon son dans l illicite comme tous ces dealers de came Le 9 4 310 puissance de frappe  qu il y ait pas d amalgame C est qui qui fait du bruit  dites moi c est quoi ce vacarme   C est S T O K A S le mec de tess que personne ricane  La Rue  pardonne pas donc forcément j ai pas d état d âme J suis dans la street  j suis dans les blocs et derrière moi j laisse deux trois cadavres Chez moi  que de la bonne substance  normal elle sort d Amsterdam Orly un phénomène  les vrais le savent on vient vous faire mal J ai le flair  je le ressens les khey comme n importe quel animal J ai les crocs  j ai envie de tout niquer les khey donc j resterai radical Le Val de Marne  Le Val de Meurtre  restera dans les annales On défonce au MIC bien pire encore qu un bon joint de zamal C est le 9 4 310  la zone est bouillante  la zone est dangereuse J dirais même  des fois même insolente  et oui khey   J arrive du 9 4 310 Mon but est de peser  vendre plus de 9 4 300 disques La rue m a légué bien plus de 9 4 300 vices Orly Sud  pour le fric c est 9 4 300 risques J arrive du 9 4 310 Mon but est de peser  vendre plus de 9 4 300 disques La rue m a légué bien plus de 9 4 300 vices Orly Sud  pour le fric c est 9 4 300 risques  Orly 94 ma favelas  300 soldats déter  à mort Tout s explique dans mon blaze  Lieutenant Jack Lars  des terres hardcores J suis d Orly mite  le collectif toujours open pour faire du tort On est encore plus vivants qu avant  Dry   Pendant que d autres nous voyaient morts  Comme dirait D R  mec déter   Orly ma ville gars on sait faire Bientôt le projet des experts  nerveux  bruyant comme un CR Orly ma ville c est Thug Life  allume quand les keufs braquent Charcle au visage ou sous la gorge  surtout si c est un petit qui braille Orly 9 4  force de frappe donc les zamels passent à la trappe On arbore l instinct de pirate  Demi Lune Zoo ma terre natale On accorde peur d importance aux falches  mais donne tout pour les vrais Comme Fian so   ou Luffy ou encore comme  En fait  je porte la rage d ici  un flow venu d ailleurs J représente mes frères dans l dîn jusqu au plus petit bicraveur Frérot t es le bienvenu dans mon hood  tout ça à tes risques et périls C est dit  Orly 9 4 310 banlieue Sud de Berryz  Berryz   J arrive du 9 4 310 Mon but est de peser  vendre plus de 9 4 300 disques La rue m a légué bien plus de 9 4 300 vices Orly Sud  pour le fric c est 9 4 300 risques   Passe le mic que j te raisonne  là c est la zone ouais la vraie Très peu d amis j en suis navré  le tiers à Fleury ou à Fresnes Zone à risque 9 4 310  un coup d Zippo et c est l incendie On arrive et tu l as senti comme la ppe fra dans un chantier Les frères écoutez les vrais  les faux finiront par se taire À suivre la tendance  t as du te faire des potes mais finalement tu t es du per Trop d amitiés rompues  on m appelle je réponds plus J ai ma belle paire de couilles en plus nos ennemis c est des  ahh  À part la hass  RAS  on s trimballe dans la rue comme des rats Trop souvent le moral à zéro  mais j reste fidèle au poste comme DR En échec scolaire  les minots s croient dans un polar Rêvent de dollar  sous Poliakov ils ont zappé l Coca Cola Les potos prennent plus des gros chiffres  mais des grosses peines et des cavales Drive by sur une grosse bécane juste à cause d une putain  cavale banane Aux lèvres même quand ça va mal  sourire kabyle  quartier de Paname Pas mal de bananes  Dry   ça jusqu à la sanction finale   J arrive du 9 4 310 Mon but est de peser  vendre plus de 9 4 300 disques La rue m a légué bien plus de 9 4 300 vices Orly Sud  pour le fric c est 9 4 300 risques\n",
      "['Un', 'balle', 'dans', 'la', 'tête', 'je', 'te', 'la', 'place', 'comme', 'Beckham', 'Je', 'vais', 'te', 'la', 'mettre', 'si', 't', 'ouvres', 'tes', 'fesses', 'Orly', 'mite', 'déboule', 'en', 'bécane', 'Mon', 'son', 'dans', 'l', 'illicite', 'comme', 'tous', 'ces', 'dealers', 'de', 'came', 'Le', '9', '4', '310', 'puissance', 'de', 'frappe', 'qu', 'il', 'y', 'ait', 'pas', 'd', 'amalgame', 'C', 'est', 'qui', 'qui', 'fait', 'du', 'bruit', 'dites', 'moi', 'c', 'est', 'quoi', 'ce', 'vacarme', 'C', 'est', 'S', 'T', 'O', 'K', 'A', 'S', 'le', 'mec', 'de', 'tess', 'que', 'personne', 'ricane', 'La', 'Rue', 'pardonne', 'pas', 'donc', 'forcément', 'j', 'ai', 'pas', 'd', 'état', 'd', 'âme', 'J', 'suis', 'dans', 'la', 'street', 'j', 'suis', 'dans', 'les', 'blocs', 'et', 'derrière', 'moi', 'j', 'laisse', 'deux', 'trois', 'cadavres', 'Chez', 'moi', 'que', 'de', 'la', 'bonne', 'substance', 'normal', 'elle', 'sort', 'd', 'Amsterdam', 'Orly', 'un', 'phénomène', 'les', 'vrais', 'le', 'savent', 'on', 'vient', 'vous', 'faire', 'mal', 'J', 'ai', 'le', 'flair', 'je', 'le', 'ressens', 'les', 'khey', 'comme', 'n', 'importe', 'quel', 'animal', 'J', 'ai', 'les', 'crocs', 'j', 'ai', 'envie', 'de', 'tout', 'niquer', 'les', 'khey', 'donc', 'j', 'resterai', 'radical', 'Le', 'Val', 'de', 'Marne', 'Le', 'Val', 'de', 'Meurtre', 'restera', 'dans', 'les', 'annales', 'On', 'défonce', 'au', 'MIC', 'bien', 'pire', 'encore', 'qu', 'un', 'bon', 'joint', 'de', 'zamal', 'C', 'est', 'le', '9', '4', '310', 'la', 'zone', 'est', 'bouillante', 'la', 'zone', 'est', 'dangereuse', 'J', 'dirais', 'même', 'des', 'fois', 'même', 'insolente', 'et', 'oui', 'khey', 'J', 'arrive', 'du', '9', '4', '310', 'Mon', 'but', 'est', 'de', 'peser', 'vendre', 'plus', 'de', '9', '4', '300', 'disques', 'La', 'rue', 'm', 'a', 'légué', 'bien', 'plus', 'de', '9', '4', '300', 'vices', 'Orly', 'Sud', 'pour', 'le', 'fric', 'c', 'est', '9', '4', '300', 'risques', 'J', 'arrive', 'du', '9', '4', '310', 'Mon', 'but', 'est', 'de', 'peser', 'vendre', 'plus', 'de', '9', '4', '300', 'disques', 'La', 'rue', 'm', 'a', 'légué', 'bien', 'plus', 'de', '9', '4', '300', 'vices', 'Orly', 'Sud', 'pour', 'le', 'fric', 'c', 'est', '9', '4', '300', 'risques', 'Orly', '94', 'ma', 'favelas', '300', 'soldats', 'déter', 'à', 'mort', 'Tout', 's', 'explique', 'dans', 'mon', 'blaze', 'Lieutenant', 'Jack', 'Lars', 'des', 'terres', 'hardcores', 'J', 'suis', 'd', 'Orly', 'mite', 'le', 'collectif', 'toujours', 'open', 'pour', 'faire', 'du', 'tort', 'On', 'est', 'encore', 'plus', 'vivants', 'qu', 'avant', 'Dry', 'Pendant', 'que', 'd', 'autres', 'nous', 'voyaient', 'morts', 'Comme', 'dirait', 'D', 'R', 'mec', 'déter', 'Orly', 'ma', 'ville', 'gars', 'on', 'sait', 'faire', 'Bientôt', 'le', 'projet', 'des', 'experts', 'nerveux', 'bruyant', 'comme', 'un', 'CR', 'Orly', 'ma', 'ville', 'c', 'est', 'Thug', 'Life', 'allume', 'quand', 'les', 'keufs', 'braquent', 'Charcle', 'au', 'visage', 'ou', 'sous', 'la', 'gorge', 'surtout', 'si', 'c', 'est', 'un', 'petit', 'qui', 'braille', 'Orly', '9', '4', 'force', 'de', 'frappe', 'donc', 'les', 'zamels', 'passent', 'à', 'la', 'trappe', 'On', 'arbore', 'l', 'instinct', 'de', 'pirate', 'Demi', 'Lune', 'Zoo', 'ma', 'terre', 'natale', 'On', 'accorde', 'peur', 'd', 'importance', 'aux', 'falches', 'mais', 'donne', 'tout', 'pour', 'les', 'vrais', 'Comme', 'Fian', 'so', 'ou', 'Luffy', 'ou', 'encore', 'comme', 'En', 'fait', 'je', 'porte', 'la', 'rage', 'd', 'ici', 'un', 'flow', 'venu', 'd', 'ailleurs', 'J', 'représente', 'mes', 'frères', 'dans', 'l', 'dîn', 'jusqu', 'au', 'plus', 'petit', 'bicraveur', 'Frérot', 't', 'es', 'le', 'bienvenu', 'dans', 'mon', 'hood', 'tout', 'ça', 'à', 'tes', 'risques', 'et', 'périls', 'C', 'est', 'dit', 'Orly', '9', '4', '310', 'banlieue', 'Sud', 'de', 'Berryz', 'Berryz', 'J', 'arrive', 'du', '9', '4', '310', 'Mon', 'but', 'est', 'de', 'peser', 'vendre', 'plus', 'de', '9', '4', '300', 'disques', 'La', 'rue', 'm', 'a', 'légué', 'bien', 'plus', 'de', '9', '4', '300', 'vices', 'Orly', 'Sud', 'pour', 'le', 'fric', 'c', 'est', '9', '4', '300', 'risques', 'Passe', 'le', 'mic', 'que', 'j', 'te', 'raisonne', 'là', 'c', 'est', 'la', 'zone', 'ouais', 'la', 'vraie', 'Très', 'peu', 'd', 'amis', 'j', 'en', 'suis', 'navré', 'le', 'tiers', 'à', 'Fleury', 'ou', 'à', 'Fresnes', 'Zone', 'à', 'risque', '9', '4', '310', 'un', 'coup', 'd', 'Zippo', 'et', 'c', 'est', 'l', 'incendie', 'On', 'arrive', 'et', 'tu', 'l', 'as', 'senti', 'comme', 'la', 'ppe', 'fra', 'dans', 'un', 'chantier', 'Les', 'frères', 'écoutez', 'les', 'vrais', 'les', 'faux', 'finiront', 'par', 'se', 'taire', 'À', 'suivre', 'la', 'tendance', 't', 'as', 'du', 'te', 'faire', 'des', 'potes', 'mais', 'finalement', 'tu', 't', 'es', 'du', 'per', 'Trop', 'd', 'amitiés', 'rompues', 'on', 'm', 'appelle', 'je', 'réponds', 'plus', 'J', 'ai', 'ma', 'belle', 'paire', 'de', 'couilles', 'en', 'plus', 'nos', 'ennemis', 'c', 'est', 'des', 'ahh', 'À', 'part', 'la', 'hass', 'RAS', 'on', 's', 'trimballe', 'dans', 'la', 'rue', 'comme', 'des', 'rats', 'Trop', 'souvent', 'le', 'moral', 'à', 'zéro', 'mais', 'j', 'reste', 'fidèle', 'au', 'poste', 'comme', 'DR', 'En', 'échec', 'scolaire', 'les', 'minots', 's', 'croient', 'dans', 'un', 'polar', 'Rêvent', 'de', 'dollar', 'sous', 'Poliakov', 'ils', 'ont', 'zappé', 'l', 'Coca', 'Cola', 'Les', 'potos', 'prennent', 'plus', 'des', 'gros', 'chiffres', 'mais', 'des', 'grosses', 'peines', 'et', 'des', 'cavales', 'Drive', 'by', 'sur', 'une', 'grosse', 'bécane', 'juste', 'à', 'cause', 'd', 'une', 'putain', 'cavale', 'banane', 'Aux', 'lèvres', 'même', 'quand', 'ça', 'va', 'mal', 'sourire', 'kabyle', 'quartier', 'de', 'Paname', 'Pas', 'mal', 'de', 'bananes', 'Dry', 'ça', 'jusqu', 'à', 'la', 'sanction', 'finale', 'J', 'arrive', 'du', '9', '4', '310', 'Mon', 'but', 'est', 'de', 'peser', 'vendre', 'plus', 'de', '9', '4', '300', 'disques', 'La', 'rue', 'm', 'a', 'légué', 'bien', 'plus', 'de', '9', '4', '300', 'vices', 'Orly', 'Sud', 'pour', 'le', 'fric', 'c', 'est', '9', '4', '300', 'risques']\n"
     ]
    }
   ],
   "source": [
    "text = df['lyrics'][0]\n",
    "print(text)\n",
    "text = clean_text(text)\n",
    "print('clean text\\n\\n\\n\\n', text)\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
